{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKKgLKCCqkkg"
      },
      "source": [
        "# Hyperparameter Tuning using Keras Tuner and Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFBbo4fFteFP"
      },
      "source": [
        "# Introduction\n",
        "KerasTuner is a general-purpose hyperparameter tuning library. It has strong integration with Keras workflows, but it isn't limited to them: you could use it to tune scikit-learn models, or anything else. In this lab, you will see how to tune model architecture, training process, and data preprocessing steps with KerasTuner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dttZqgByqsSX"
      },
      "source": [
        "There are some advanced hyperparameter tuning algorithms, including Random serach tuner, Bayesian hyperparameter optimization, Hyperband, Sklearn tuner. All of these are implemented inside the [keras tuner package](https://keras.io/keras_tuner/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M8paWJjr7cX"
      },
      "source": [
        "### Advantages of Keras Tuner\n",
        "\n",
        "\n",
        "1.   Ease of use\n",
        "\n",
        "2. Integrates into your existing deep learning training pipeline with minimal code changes\n",
        "3. Implements novel hyperparameter tuning algorithms\n",
        "4. Can boost accuracy with minimal effort on your part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZAx3sUcux2P",
        "outputId": "59dd2ed8-f0eb-43c9-c235-ff1768a83153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 135 kB 14.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 69.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkfpGgvbu1L3"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(\n",
        "        layers.Dense(\n",
        "            # Define the hyperparameter.\n",
        "            units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1LNUxbmu5xn",
        "outputId": "0040ff6b-77d6-451c-d796-1e1a285226a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f74209c6d10>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#quickly test if the model builds successfully.\n",
        "import keras_tuner\n",
        "\n",
        "build_model(keras_tuner.HyperParameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-T3Ext7vUFf"
      },
      "outputs": [],
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,  #The total number of trials to run during the search.\n",
        "    executions_per_trial=2, #The number of models that should be built and fit for each trial.\n",
        "    overwrite=True, #Control whether to overwrite the previous results, overwrite=True to start a new search and ignore any previous results.\n",
        "    directory=\"my_dir\",#A path to a directory for storing the search results.\n",
        "    project_name=\"helloworld\", #The name of the sub-directory in the directory.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZUAVNgrvW6g",
        "outputId": "aa8cc7f0-9488-42a9-fde6-ffed40e5a8cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 1\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n"
          ]
        }
      ],
      "source": [
        "tuner.search_space_summary() #print a summary of the search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t15dCsT_vasG",
        "outputId": "41801f90-d75a-4b87-e005-66deaed6571e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#Before starting the search, let's prepare the MNIST dataset.\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "(x, y), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x[:-10000]\n",
        "x_val = x[-10000:]\n",
        "y_train = y[:-10000]\n",
        "y_val = y[-10000:]\n",
        "\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
        "x_val = np.expand_dims(x_val, -1).astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.0\n",
        "\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CopCCHNrvvkO"
      },
      "source": [
        "Then, start the search for the best hyperparameter configuration. All the arguments passed to search is passed to model.fit() in each execution. Remember to pass validation_data to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFLTleCRvdFO",
        "outputId": "d0c06016-777b-4020-fb97-af2c0be917b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 00m 42s]\n",
            "val_accuracy: 0.9748000204563141\n",
            "\n",
            "Best val_accuracy So Far: 0.9748000204563141\n",
            "Total elapsed time: 00h 01m 24s\n"
          ]
        }
      ],
      "source": [
        "tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcGH7sh_w2jG",
        "outputId": "7c020a3f-a929-4a0c-ce48-f9e5e539410a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in my_dir/helloworld\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7f741b749050>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "Score: 0.9748000204563141\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "Score: 0.9711999893188477\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "Score: 0.9532999992370605\n"
          ]
        }
      ],
      "source": [
        "tuner.results_summary() # print a summary of the search results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-fMWMgfty4e"
      },
      "source": [
        "## Query the results\n",
        "When search is over, you can retrieve the best model(s). The model is saved at its best performing epoch evaluated on the validation_data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47745G3cwqe-",
        "outputId": "2ad88ffa-5fdd-4d64-f509-e4f6dd0614bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 480)               376800    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                4810      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 381,610\n",
            "Trainable params: 381,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Get the top 2 models.\n",
        "models = tuner.get_best_models(num_models=2)\n",
        "best_model = models[0]\n",
        "# Build the model.\n",
        "# Needed for `Sequential` without specified `input_shape`.\n",
        "best_model.build(input_shape=(None, 28, 28))\n",
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsMrIynNxGc2"
      },
      "source": [
        "## Retrain the model\n",
        "If you want to train the model with the entire dataset, you may retrieve the best hyperparameters and retrain the model by yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWoUIAKsxIQ-",
        "outputId": "70771555-0295-49a8-df3b-c198eddfe68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2042 - accuracy: 0.9399\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f741996a6d0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the top 2 hyperparameters.\n",
        "best_hps = tuner.get_best_hyperparameters(5)\n",
        "# Build the model with the best hp.\n",
        "model = build_model(best_hps[0])\n",
        "# Fit with the entire dataset.\n",
        "x_all = np.concatenate((x_train, x_val))\n",
        "y_all = np.concatenate((y_train, y_val))\n",
        "model.fit(x=x_all, y=y_all, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vK3hjlazLvv"
      },
      "outputs": [],
      "source": [
        "# <put your resolve here >\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "    layer_num = hp.Int(\"layer\", min_value=1, max_value=9, step=1)\n",
        "    for _ in range(layer_num):\n",
        "      model.add(\n",
        "          layers.Dense(\n",
        "              units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
        "              activation=hp.Choice(\"activation\", values = [\"relu\", \"tanh\"]),\n",
        "          )\n",
        "      )\n",
        "\n",
        "    model.add(keras.layers.Dropout(0.15))\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLpGyob13VaF"
      },
      "outputs": [],
      "source": [
        "# <put your resolve here >\n",
        "tuner = keras_tuner.Hyperband(build_model,objective='val_accuracy',max_epochs=5,factor=3,directory=\"/content/keras_tuner_test\",project_name= \"kt_hyperband\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_koULhw4eBG"
      },
      "outputs": [],
      "source": [
        "# <put your resolve here >\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x, y), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train, X_rem, y_train, y_rem = train_test_split(x,y, train_size=0.8)\n",
        "\n",
        "# Now since we want the valid and test size to be equal (10% each of overall data). \n",
        "# we have to define valid_size=0.5 (that is 50% of remaining data)\n",
        "test_size = 0.5\n",
        "x_val, x_test, y_val, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
        "\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
        "x_val = np.expand_dims(x_val, -1).astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.0\n",
        "\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG_yIr8B43QV",
        "outputId": "e29ce436-0d74-47d3-f721-f315fa392c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 02m 23s]\n",
            "val_accuracy: 0.95333331823349\n",
            "\n",
            "Best val_accuracy So Far: 0.9703333377838135\n",
            "Total elapsed time: 00h 10m 44s\n"
          ]
        }
      ],
      "source": [
        "# <put your resolve here >\n",
        "tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeGAeAjIrbxY",
        "outputId": "e2f6bb36-a9b9-4b46-a21c-ac7d43a45fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in /content/keras_tuner_test/kt_hyperband\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7f7416a71cd0>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "layer: 2\n",
            "units: 320\n",
            "activation: tanh\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 5\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.9703333377838135\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "layer: 6\n",
            "units: 480\n",
            "activation: relu\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 5\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0001\n",
            "Score: 0.9671666622161865\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "layer: 6\n",
            "units: 480\n",
            "activation: relu\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.9614999890327454\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "layer: 7\n",
            "units: 480\n",
            "activation: tanh\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 5\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0004\n",
            "Score: 0.9580000042915344\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "layer: 7\n",
            "units: 480\n",
            "activation: tanh\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 5\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.95333331823349\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "layer: 7\n",
            "units: 480\n",
            "activation: tanh\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.9483333230018616\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "layer: 8\n",
            "units: 96\n",
            "activation: tanh\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.9296666383743286\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "layer: 2\n",
            "units: 64\n",
            "activation: tanh\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.9233333468437195\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "layer: 8\n",
            "units: 480\n",
            "activation: relu\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 5\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.8345000147819519\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "layer: 9\n",
            "units: 224\n",
            "activation: relu\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.6604999899864197\n"
          ]
        }
      ],
      "source": [
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPlUTMj4bJ3h",
        "outputId": "4989b25b-0d8c-481d-caf4-8faf873116be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 320)               251200    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 320)               102720    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 320)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                3210      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 357,130\n",
            "Trainable params: 357,130\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
          ]
        }
      ],
      "source": [
        "# Get the top 2 models.\n",
        "models = tuner.get_best_models(num_models=2)\n",
        "best_model = models[0]\n",
        "# Build the model.\n",
        "# Needed for `Sequential` without specified `input_shape`.\n",
        "best_model.build(input_shape=(None, 28, 28))\n",
        "best_model.summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
